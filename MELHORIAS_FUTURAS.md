# üöÄ ROADMAP DE MELHORIAS - MERCADO INSIGHTS

**Baseado em:** Pesquisa de comparadores profissionais + An√°lise t√©cnica  
**√öltima atualiza√ß√£o:** 09/02/2026

---

## üéØ VIS√ÉO GERAL

Este documento lista melhorias **n√£o urgentes** que podem aumentar performance, reduzir custos e melhorar UX.

**Todas as funcionalidades principais j√° funcionam.** Estas s√£o otimiza√ß√µes incrementais.

---

## üìä PRIORIZA√á√ÉO POR IMPACTO

| Melhoria | Impacto | Esfor√ßo | Prioridade | Quando |
|----------|---------|---------|-----------|--------|
| **Multiget** | üü¢ Alto | üü° Baixo | ‚≠ê‚≠ê‚≠ê | Curto prazo |
| **Cache b√°sico** | üü¢ Alto | üü° M√©dio | ‚≠ê‚≠ê‚≠ê | Curto prazo |
| **Rate limiting** | üü° M√©dio | üü° Baixo | ‚≠ê‚≠ê | M√©dio prazo |
| **Backoff exponencial** | üü° M√©dio | üü¢ Baixo | ‚≠ê‚≠ê | M√©dio prazo |
| **Hist√≥rico de pre√ßos** | üü° M√©dio | üü† Alto | ‚≠ê | Longo prazo |
| **Webhooks ML** | üü° M√©dio | üü† Alto | ‚≠ê | Longo prazo |
| **Certifica√ß√£o ML** | üü¢ Alto | üî¥ Muito alto | ‚ö†Ô∏è | S√≥ se GMV > $10k/m√™s |
| **Proxies** | üîµ Baixo | üü† Alto | ‚ùå | Evitar |
| **Scraping** | üîµ Baixo | üî¥ Muito alto | ‚ùå | Evitar |

---

## üöÄ FASE 1: OTIMIZA√á√ÉO B√ÅSICA (1-2 SEMANAS)

### 1.1. Implementar Multiget

**Problema atual:**
```python
# Buscar 20 concorrentes = 20 requisi√ß√µes
for item_id in competitor_ids:
    detail = get_item_by_id(token, item_id)  # 1 request cada
```

**Solu√ß√£o:**
```python
# Buscar 20 concorrentes = 1 requisi√ß√£o
def get_multiple_items_optimized(access_token: str, item_ids: List[str]) -> List[dict]:
    """Busca at√© 20 itens de uma vez usando /items?ids=..."""
    if not item_ids:
        return []
    
    # API ML aceita at√© 20 IDs por vez
    chunks = [item_ids[i:i+20] for i in range(0, len(item_ids), 20)]
    all_items = []
    
    for chunk in chunks:
        ids_str = ",".join(chunk)
        params = {
            "ids": ids_str,
            "attributes": "id,price,title,sold_quantity,permalink,thumbnail"  # Reduz payload
        }
        headers = {"Authorization": f"Bearer {access_token}"}
        resp = requests.get(f"{ML_API}/items", headers=headers, params=params, timeout=15)
        
        if resp.status_code == 200:
            raw = resp.json()  # [{code: 200, body: {...}}, ...]
            for obj in raw:
                if obj.get("code") == 200 and "body" in obj:
                    all_items.append(obj["body"])
        
    return all_items
```

**Impacto:**
- ‚úÖ **20x mais r√°pido** para listar concorrentes
- ‚úÖ **90% menos requisi√ß√µes** (economia de rate limit)
- ‚úÖ **Menor payload** com sele√ß√£o de campos

**Onde aplicar:**
- `/api/ml/competitors` (GET) - listar concorrentes cadastrados

---

### 1.2. Cache em Mem√≥ria

**Problema atual:**
```python
# Toda vez que lista concorrentes, busca na API ML (lento)
for r in rows:
    detail = get_item_by_id(token, r.item_id)  # API call
```

**Solu√ß√£o:**
```python
from datetime import datetime, timedelta

# Cache global (ou usar Redis em produ√ß√£o)
_items_cache = {}

def get_item_cached(access_token: str, item_id: str, ttl_seconds: int = 7200):
    """Busca item com cache de 2 horas (padr√£o)."""
    now = datetime.utcnow()
    cache_key = f"item:{item_id}"
    
    # Verifica cache
    if cache_key in _items_cache:
        cached = _items_cache[cache_key]
        if cached["expires_at"] > now:
            return cached["data"]  # Usa cache ‚úÖ
    
    # Cache expirado ou n√£o existe, busca na API
    item = get_item_by_id(access_token, item_id)
    
    # Armazena no cache (apenas se sucesso)
    if item and not item.get("error"):
        _items_cache[cache_key] = {
            "data": item,
            "expires_at": now + timedelta(seconds=ttl_seconds)
        }
    
    return item
```

**TTL recomendado:**
- T√≠tulo, categoria: 24 horas (raramente muda)
- Pre√ßo, estoque: 2-4 horas (pode mudar)
- Vendidos: 1 hora (atualiza frequentemente)

**Impacto:**
- ‚úÖ **80% menos requisi√ß√µes** √† API ML
- ‚úÖ **3x mais r√°pido** para usu√°rio
- ‚úÖ **Reduz rate limit** significativamente

---

### 1.3. Sele√ß√£o de Campos

**Problema atual:**
```python
# Retorna ~5KB por item (inclui tudo: imagens, varia√ß√µes, etc.)
GET /items/MLB123
```

**Solu√ß√£o:**
```python
# Retorna ~500B (apenas o necess√°rio)
GET /items/MLB123?attributes=id,price,title,sold_quantity,permalink,thumbnail
```

**Implementa√ß√£o:**
```python
def get_item_by_id(access_token, item_id, attributes=None):
    params = {}
    if attributes:
        params["attributes"] = ",".join(attributes)
    
    resp = requests.get(f"{ML_API}/items/{item_id}", 
                       headers=headers, 
                       params=params,  # ‚úÖ Adiciona sele√ß√£o
                       timeout=15)
```

**Impacto:**
- ‚úÖ **10x menos dados** transferidos
- ‚úÖ **Resposta 2x mais r√°pida**
- ‚úÖ **Economia de banda**

---

## ‚öôÔ∏è FASE 2: CONFIABILIDADE (1-3 MESES)

### 2.1. Rate Limiting com Token Bucket

**Problema:** Sem controle de quantas requisi√ß√µes fazemos √† API ML.

**Solu√ß√£o:**
```python
class TokenBucket:
    """Limita requisi√ß√µes √† API ML."""
    def __init__(self, capacity=100, refill_rate=10):
        self.capacity = capacity  # m√°x 100 tokens
        self.tokens = capacity
        self.refill_rate = refill_rate  # +10 tokens/segundo
        self.last_refill = datetime.utcnow()
    
    def consume(self, tokens=1):
        """Tenta consumir N tokens. Retorna True se OK."""
        self._refill()
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False  # Rate limit atingido
    
    def _refill(self):
        """Adiciona tokens ao longo do tempo."""
        now = datetime.utcnow()
        elapsed = (now - self.last_refill).total_seconds()
        tokens_to_add = int(elapsed * self.refill_rate)
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now

# Uso global
ml_rate_limiter = TokenBucket(capacity=100, refill_rate=10)

def safe_ml_request(url, headers):
    if not ml_rate_limiter.consume():
        raise HTTPException(429, "Rate limit interno. Aguarde alguns segundos.")
    return requests.get(url, headers=headers, timeout=15)
```

**Impacto:**
- ‚úÖ Evita bloqueio da API ML
- ‚úÖ Distribui requisi√ß√µes ao longo do tempo
- ‚úÖ Permite bursts controlados

---

### 2.2. Backoff Exponencial para 429

**Problema:** Se ML retornar 429 (Too Many Requests), falhamos imediatamente.

**Solu√ß√£o:**
```python
def fetch_with_retry(url, headers, max_retries=3):
    """Tenta requisi√ß√£o com retry exponencial."""
    for attempt in range(max_retries):
        resp = requests.get(url, headers=headers, timeout=15)
        
        if resp.status_code == 200:
            return resp.json()
        
        elif resp.status_code == 429:
            # Respeita Retry-After se fornecido
            retry_after = int(resp.headers.get('Retry-After', 2 ** attempt))
            _log.warning(f"Rate limit 429, aguardando {retry_after}s...")
            time.sleep(retry_after)
            continue
        
        else:
            # Outro erro, n√£o retry
            raise Exception(f"Erro {resp.status_code}")
    
    raise Exception("Max retries atingido")
```

**Impacto:**
- ‚úÖ Recupera√ß√£o autom√°tica de rate limit
- ‚úÖ Menos erros para o usu√°rio
- ‚úÖ Melhor UX

---

### 2.3. Circuit Breaker

**Problema:** Se API ML estiver inst√°vel, continuamos tentando (desperdi√ßa recursos).

**Solu√ß√£o:**
```python
class CircuitBreaker:
    """Para requisi√ß√µes se API est√° falhando consistentemente."""
    def __init__(self, threshold=5, timeout=60):
        self.threshold = threshold  # Erros consecutivos para abrir
        self.timeout = timeout  # Segundos at√© tentar novamente
        self.failures = 0
        self.opened_at = None
        self.state = "closed"  # closed | open | half-open
    
    def call(self, func, *args, **kwargs):
        if self.state == "open":
            # Verifica se pode tentar novamente
            if datetime.utcnow() > self.opened_at + timedelta(seconds=self.timeout):
                self.state = "half-open"  # Tentativa √∫nica
            else:
                raise Exception("Circuit breaker aberto. API ML temporariamente indispon√≠vel.")
        
        try:
            result = func(*args, **kwargs)
            self.failures = 0
            self.state = "closed"
            return result
        except Exception as e:
            self.failures += 1
            if self.failures >= self.threshold:
                self.state = "open"
                self.opened_at = datetime.utcnow()
                _log.error(f"Circuit breaker aberto ap√≥s {self.failures} falhas")
            raise e
```

**Impacto:**
- ‚úÖ Evita sobrecarga quando API ML est√° fora
- ‚úÖ Recupera√ß√£o autom√°tica
- ‚úÖ Mensagem clara ao usu√°rio

---

## üìà FASE 3: ESCALABILIDADE (6+ MESES)

### 3.1. Hist√≥rico de Pre√ßos

**Problema:** N√£o sabemos como o pre√ßo dos concorrentes mudou ao longo do tempo.

**Solu√ß√£o:**
```python
class PriceHistory(Base):
    __tablename__ = "price_history"
    
    id = Column(Integer, primary_key=True)
    item_id = Column(String(32), nullable=False, index=True)
    price = Column(Float, nullable=False)
    sold_quantity = Column(Integer, nullable=True)
    available_quantity = Column(Integer, nullable=True)
    recorded_at = Column(DateTime, default=datetime.utcnow, index=True)

# Job peri√≥dico (a cada 4h)
def record_competitor_prices():
    """Registra pre√ßo atual de todos os concorrentes."""
    db = SessionLocal()
    items = db.query(CompetitorItem).all()
    for item in items:
        detail = get_item_cached(token, item.item_id)
        if detail and not detail.get("error"):
            db.add(PriceHistory(
                item_id=item.item_id,
                price=detail.get("price"),
                sold_quantity=detail.get("sold_quantity"),
                available_quantity=detail.get("available_quantity")
            ))
    db.commit()
```

**Impacto:**
- ‚úÖ An√°lise de tend√™ncias (pre√ßo subindo/caindo)
- ‚úÖ Alertas de mudan√ßa de pre√ßo
- ‚úÖ Gr√°ficos de hist√≥rico

---

### 3.2. Atualiza√ß√£o Autom√°tica (Cron Job)

**Problema:** Dados de concorrentes s√≥ atualizam quando usu√°rio acessa.

**Solu√ß√£o:**
```python
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()

@scheduler.scheduled_job('interval', hours=4)
def update_all_competitors():
    """Atualiza dados de todos os concorrentes a cada 4h."""
    db = SessionLocal()
    items = db.query(CompetitorItem).all()
    
    # Agrupa por usu√°rio para usar token correto
    by_user = {}
    for item in items:
        if item.user_id not in by_user:
            by_user[item.user_id] = []
        by_user[item.user_id].append(item.item_id)
    
    for user_id, item_ids in by_user.items():
        user = db.query(User).get(user_id)
        token = get_valid_ml_token(user)
        if not token:
            continue
        
        # Usa Multiget (at√© 20 por vez)
        items_data = get_multiple_items_optimized(token.access_token, item_ids)
        
        # Registra hist√≥rico
        for item_data in items_data:
            db.add(PriceHistory(
                item_id=item_data["id"],
                price=item_data.get("price"),
                sold_quantity=item_data.get("sold_quantity")
            ))
    
    db.commit()
    _log.info(f"Atualiza√ß√£o autom√°tica: {len(items)} concorrentes atualizados")

# Inicia scheduler no startup
scheduler.start()
```

**Impacto:**
- ‚úÖ Dados sempre atualizados
- ‚úÖ Usu√°rio v√™ informa√ß√µes recentes sem esperar
- ‚úÖ Hist√≥rico autom√°tico

---

### 3.3. Cache Persistente (Redis)

**Problema:** Cache em mem√≥ria se perde quando backend reinicia.

**Solu√ß√£o:**
```python
import redis

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

def get_item_redis_cached(access_token, item_id, ttl=7200):
    """Cache com Redis (persiste entre restarts)."""
    cache_key = f"ml:item:{item_id}"
    
    # Verifica cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Busca na API
    item = get_item_by_id(access_token, item_id)
    
    # Armazena (apenas se sucesso)
    if item and not item.get("error"):
        redis_client.setex(cache_key, ttl, json.dumps(item))
    
    return item
```

**Impacto:**
- ‚úÖ Cache sobrevive a restarts
- ‚úÖ Compartilhado entre inst√¢ncias (horizontal scaling)
- ‚úÖ TTL autom√°tico (Redis gerencia expira√ß√£o)

**Custo:**
- Railway Redis: ~$5/m√™s

---

## üéì FASE 4: AVAN√áADO (APENAS SE NECESS√ÅRIO)

### 4.1. Certifica√ß√£o ML

**Quando considerar:**
- GMV mensal > $10k (requisito m√≠nimo)
- Precisa de SLA de suporte
- Busca p√∫blica bloqueada (403 persistente)

**N√≠veis:**
| N√≠vel | GMV/m√™s | Requisitos | Benef√≠cios |
|-------|---------|-----------|------------|
| Certified | $10k | Seguran√ßa 65% | Suporte via ticket |
| Silver | $100k | Seguran√ßa 80% | Listagem App-Store |
| Gold | $1M | Seguran√ßa 90% | SLA reduzido (10 dias) |
| Platinum | $10M | Seguran√ßa 95% | Wishlist de recursos |

**Nosso caso:**
- ‚ö†Ô∏è **N√£o recomendado agora** (escala pequena)
- ‚úÖ API sem certifica√ß√£o √© suficiente
- ‚úÖ Adicionar por link/ID funciona perfeitamente

---

### 4.2. Proxies (N√ÉO RECOMENDADO)

**Quando usar:**
- Bloqueio persistente por IP
- Necess√°rio scraping (n√£o nosso caso)
- Geo-targeting (n√£o nosso caso)

**Custo:**
- Proxies residenciais: $20-100/m√™s (Oxylabs, Bright Data)
- Rota√ß√£o de IP: $49/m√™s (ScraperAPI)

**Nosso caso:**
- ‚ùå **N√£o necess√°rio**
- API oficial funciona sem proxies
- Economia: $20-100/m√™s

---

### 4.3. Scraping (√öLTIMO RECURSO)

**Quando usar:**
- Dados n√£o dispon√≠veis via API
- Certifica√ß√£o ML imposs√≠vel
- Necess√°rio em tempo real absoluto

**Riscos:**
- ‚ùå Bloqueio de IP/conta
- ‚ùå Viola√ß√£o de termos de uso
- ‚ùå Manuten√ß√£o constante (HTML muda)
- ‚ùå CAPTCHA e anti-bot
- ‚ùå Custo alto (proxies + desenvolvimento)

**Nosso caso:**
- ‚ùå **Evitar completamente**
- API oficial atende todas as necessidades
- Adicionar por ID/link funciona sem scraping

---

## üìä COMPARA√á√ÉO DE ABORDAGENS

| Abordagem | Custo/m√™s | Confiabilidade | Manuten√ß√£o | Legal | Nossa nota |
|-----------|-----------|----------------|-----------|-------|-----------|
| **API oficial** | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **API + Multiget** | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **API + Cache** | $0-5 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Certifica√ß√£o ML** | $0* | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚≠ê‚≠ê‚≠ê |
| **Proxies** | $20-100 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ö†Ô∏è | ‚≠ê |
| **Scraping** | $50-200 | ‚≠ê‚≠ê | ‚≠ê | ‚ùå | ‚ùå |

**\*Certifica√ß√£o requer GMV $10k+/m√™s**

**Recomenda√ß√£o:** API oficial + Multiget + Cache.

---

## üß™ TESTES ANTES DE IMPLEMENTAR

### Antes de Multiget:
```bash
# Medir tempo atual
time curl "https://mercadoinsights.online/api/ml/competitors" \
  -H "Authorization: Bearer TOKEN"

# Esperado: ~5-10s para 10 concorrentes
```

### Depois de Multiget:
```bash
# Medir tempo otimizado
time curl "https://mercadoinsights.online/api/ml/competitors" \
  -H "Authorization: Bearer TOKEN"

# Esperado: <1s para 10 concorrentes
```

---

## üìö REFER√äNCIAS T√âCNICAS

### Artigos e Guias
- [ML API Best Practices](https://developers.mercadolivre.com.br/pt_br/boas-praticas-de-apis)
- [Rate Limiting Strategies](https://cloud.google.com/architecture/rate-limiting-strategies-techniques)
- [Cache Invalidation Patterns](https://martinfowler.com/bliki/TwoHardThings.html)

### Ferramentas
- [APScheduler](https://apscheduler.readthedocs.io/) - Cron jobs em Python
- [Redis](https://redis.io/) - Cache persistente
- [Token Bucket Algorithm](https://en.wikipedia.org/wiki/Token_bucket)

---

## ‚úÖ CONCLUS√ÉO

### Nossa estrat√©gia (API oficial + ID/link) √© s√≥lida ‚úì

**Validado por:**
- ‚úÖ Comparadores profissionais usam a mesma abordagem
- ‚úÖ Escala atual n√£o justifica scraping
- ‚úÖ API ML atende nossas necessidades

**Pr√≥ximas otimiza√ß√µes:**
1. **Multiget** (maior impacto, baixo esfor√ßo) ‚Üê PRIORIDADE
2. **Cache** (reduz custos, melhora UX)
3. **Rate limiting** (evita bloqueio)

**Evitar:**
- ‚ùå Scraping (risco, custo, manuten√ß√£o)
- ‚ùå Proxies (custo desnecess√°rio)
- ‚ö†Ô∏è Certifica√ß√£o ML (s√≥ se GMV > $10k/m√™s)

---

**üöÄ Implemente as melhorias da Fase 1 nas pr√≥ximas 1-2 semanas.**

Ver c√≥digo de exemplo completo em: `ESTRATEGIA_CONCORRENCIA.md`
